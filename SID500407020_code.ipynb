{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## was in decomposition.py\n",
    "import numpy as np\n",
    "import operator as op\n",
    "\n",
    "\n",
    "def var_covar_matrix(X, mean=None, axis=0):\n",
    "    assert len(X.shape) == 2, 'must operate on a matrix of 2 dimensions'\n",
    "    if axis == 1:  # calculate on transpose\n",
    "        return var_covar_matrix(X.T, mean=mean)\n",
    "    elif axis != 0:\n",
    "        raise ValueError('axis must 0 or 1')\n",
    "    # axis is now == 0\n",
    "\n",
    "    if mean is None:\n",
    "        mean = np.mean(X, axis=axis)\n",
    "    diff = X - mean\n",
    "\n",
    "    # sum of outer products for each vector divided by the number of vectors\n",
    "    rv = (diff.T @ diff) / X.shape[0]\n",
    "    return rv\n",
    "\n",
    "\n",
    "class Transformation:\n",
    "    \"\"\"Abstract Base Class for transformation objects.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"fit the transformation to data X\n",
    "\n",
    "        :param X: input data (first dimension should represent rows)\n",
    "        :param y: optional - data labels\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This is an abstract method')\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"transform X into the representation domain.\n",
    "\n",
    "        Raises an exception if fit has not first been called.\n",
    "\n",
    "        :param X: input data (first dimension should represent rows)\n",
    "        :param y: optional - data labels\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This is an abstract method')\n",
    "\n",
    "    def inverse_transform(self, W, y=None):\n",
    "        \"\"\"Transform the representation back into the data domain.\n",
    "\n",
    "        :param X: input data (first dimension should represent rows)\n",
    "        :param y: optional - data labels\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This is an abstract method')\n",
    "\n",
    "\n",
    "class IdentityTransformation(Transformation):\n",
    "    \"\"\"IdentityTransformation. A transformation that does nothing\n",
    "\n",
    "    useful for testing purposes\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, W):\n",
    "        return W\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'IdentityTransformation()'\n",
    "\n",
    "\n",
    "class PCA(Transformation):\n",
    "\n",
    "    def __init__(self, components=None, normalize=True):\n",
    "        self.normalize = normalize\n",
    "        # assumes normalized data (mean of 0 over all axes, sd of 1)\n",
    "        self.k = components\n",
    "        self.normalize = normalize\n",
    "        self._metavalues = dict(\n",
    "                variances = None\n",
    "                )\n",
    "\n",
    "    def _norm(self, X):\n",
    "        return (X - self.means) / self.sds\n",
    "\n",
    "    def _inv_norm(self, W):\n",
    "        return (W * self.sds) + self.means\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if self.k is None:\n",
    "            self.k = X.shape[1]\n",
    "        if self.normalize:\n",
    "            self.means = np.mean(X, axis=0)\n",
    "            self.sds = np.std(X, axis=0)\n",
    "        else:\n",
    "            self.means = np.zeros(X.shape[1])\n",
    "            self.sds = np.ones(X.shape[1])\n",
    "\n",
    "        X = self._norm(X)\n",
    "\n",
    "        cov = var_covar_matrix(X, mean=np.zeros(X.shape[1]))\n",
    "        val, vec = np.linalg.eigh(cov)  # cov is symmetric, so eigh performs better\n",
    "\n",
    "        # vecs columns are eigenvectors\n",
    "        pairs = sorted(zip(val, vec.T), key=op.itemgetter(0), reverse=True)\n",
    "        self._metavalues['variances'] = np.array(list(map(op.itemgetter(0), pairs)))\n",
    "        self.components = np.array(list(map(op.itemgetter(1), pairs[:self.k]))).T\n",
    "        self.inverse_transform_components = self.components.T\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self._norm(X) @ self.components\n",
    "\n",
    "    def inverse_transform(self, W):\n",
    "        return self._inv_norm(W @ self.inverse_transform_components)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'PCA({self.k}, {self.normalize})'\n",
    "\n",
    "\n",
    "class NMF:\n",
    "\n",
    "    def __init__(self, components, stop_threshold=0.01, max_iter=200, initial_dictionary=None, image_shape=None):\n",
    "        if initial_dictionary is not None:\n",
    "            initial_dictionary = initial_dictionary.copy()\n",
    "        self._metavalues = dict(\n",
    "            name='L2 Norm NMF',\n",
    "            training_loss=[],\n",
    "            training_residue=[],\n",
    "            components=components,\n",
    "            stop_threshold=stop_threshold,\n",
    "            max_iter=max_iter,\n",
    "            initial_dictionary=initial_dictionary,\n",
    "            image_shape=image_shape,\n",
    "        )\n",
    "        self._dictionary = None\n",
    "        self._inverse_dictionary = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, initial_representation=None):\n",
    "        \"\"\" Assumes first dimension of X represents rows of data \"\"\"\n",
    "\n",
    "        if self._metavalues['image_shape'] is None:\n",
    "            # initialise default image shape if was not previously assigned\n",
    "            self._metavalues['image_shape'] = X.shape[1:]\n",
    "        else:\n",
    "            # sanity checks\n",
    "            assert X.shape[1:] == self._metavalues['image_shape'], ('input data does '\n",
    "                                                                    'not match expected shape')\n",
    "\n",
    "        # reshape the data to be vectors instead of images (if not already reshaped)\n",
    "        n: int = X.shape[0]\n",
    "        p: int = np.product(X.shape[1:])\n",
    "        k: int = self._metavalues['components']\n",
    "        X: np.ndarray = NMF._reshape_forward(X)\n",
    "        assert X.shape == (p, n)\n",
    "\n",
    "        # n - number of input images\n",
    "        # p - dimensionality of population space\n",
    "        # k - number of components\n",
    "\n",
    "        # X shape (p, n)\n",
    "        # D shape (p, k)\n",
    "        # R shape (k, n)\n",
    "\n",
    "        # initialise the learning dictionary if not already initialised\n",
    "        if self._metavalues['initial_dictionary'] is None:\n",
    "            self._metavalues['initial_dictionary'] = np.random.rand(p, k)\n",
    "        else:\n",
    "            assert self._metavalues['initial_dictionary'].shape == (p, k)\n",
    "\n",
    "        # initialize dictionary if not already done.\n",
    "        if self._dictionary is None:\n",
    "            self._dictionary = self._metavalues['initial_dictionary'].copy()\n",
    "\n",
    "        # initialize representation\n",
    "        if initial_representation is None:\n",
    "            R: np.ndarray = np.random.rand(k, n)\n",
    "        else:\n",
    "            R: np.ndarray = initial_representation.copy()\n",
    "            assert R.shape == (k, n)\n",
    "\n",
    "        D: np.ndarray = self._dictionary  # alias for readability.\n",
    "\n",
    "        # toggle optimizing between D and R\n",
    "        # start with updating 'R'\n",
    "        optim = 'R'\n",
    "\n",
    "        # marker for different calls\n",
    "        self._metavalues['training_loss'].append(None)\n",
    "        self._metavalues['training_residue'].append(None)\n",
    "\n",
    "        # fit the data\n",
    "        for iteration in range(self._metavalues['max_iter'] * 2):  # *2 to account for alternation\n",
    "            # this section follows section 2.7 of the accompanied documentation in\n",
    "            # ../papers/Robust Nonnegative Matrix Factorization using L21 Norm 2011.pdf\n",
    "\n",
    "            # only collect the loss after D has been updated\n",
    "            if optim == 'D':\n",
    "                diffs = X - D @ R\n",
    "                loss = l2_norm(diffs)\n",
    "                residue = np.linalg.norm(diffs)\n",
    "\n",
    "                # keep these for later\n",
    "                self._metavalues['training_loss'].append(loss)\n",
    "                self._metavalues['training_residue'].append(residue)\n",
    "\n",
    "                # computing if stopping condition is met\n",
    "                if iteration >= 2:\n",
    "                    previous_loss = self._metavalues['training_loss'][-1]\n",
    "                    current_loss = self._metavalues['training_loss'][-2]\n",
    "                    relative_improvement= - (previous_loss - current_loss) / previous_loss\n",
    "                    if relative_improvement < self._metavalues['stop_threshold']:\n",
    "                        optim = 'stop'\n",
    "\n",
    "            if optim == 'D':\n",
    "                optim = 'R'  # toggle for next time\n",
    "                D *= (X @ R.T) / (D @ R @ R.T)\n",
    "            elif optim == 'R':\n",
    "                optim = 'D'\n",
    "                R *= (D.T @ X) / (D.T @ D @ R)\n",
    "\n",
    "            elif optim == 'stop':\n",
    "                break\n",
    "            else:\n",
    "                assert 0, 'optim not recognised'\n",
    "\n",
    "        self._inverse_dictionary = np.linalg.inv(D.T @ D) @ D.T\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\" Transform X into its representation\n",
    "\n",
    "        :param X: row matrix/tensor of same shape as training time representing\n",
    "            data. If there are n images of size 10x5, X should be of shape\n",
    "            (n, 10, 5) or (n, 50) (depending on what was passed at training\n",
    "            time)\n",
    "        :return: row matrix (n, k) representing X.\n",
    "\n",
    "        Returns a row oriented matrix of representation vectors of X\n",
    "        \"\"\"\n",
    "        return (self._inverse_dictionary @ NMF._reshape_forward(X)).T\n",
    "\n",
    "    def inverse_transform(self, R):\n",
    "        \"\"\" Transform representations of X back into X.\n",
    "\n",
    "        :param R: row oriented matrix of representation vectors\n",
    "        :return: row matrix/tensor of the same shape as input (barring first\n",
    "        dimension)\n",
    "        \"\"\"\n",
    "        return self._reshape_backward(self._dictionary @ R.T)\n",
    "\n",
    "    def get_metavalues(self):\n",
    "        \"\"\" NMF.get_metavalues\n",
    "\n",
    "        returns a dict with the following attributes:\n",
    "        'name' : name of the algorithm\n",
    "        'training_loss' : loss of the algorithm at each iteration during\n",
    "            training\n",
    "        'components' : dimensionality of the representation vectors.\n",
    "        'max_iter' : maximum number of iterations in training\n",
    "        \"\"\"\n",
    "        return self._metavalues\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_forward(mat):\n",
    "        \"\"\" transpose a row matrix or tensor to a column matrix \"\"\"\n",
    "        if len(mat.shape) == 3:\n",
    "            return mat.reshape(mat.shape[0], -1).T\n",
    "        if len(mat.shape) == 2:\n",
    "            return mat.T\n",
    "        raise ValueError(f'expected a 2 or 3 dimensional matrix. Got a matrix '\n",
    "                         'of shape {mat.shape}')\n",
    "\n",
    "    def _reshape_backward(self, mat):\n",
    "        \"\"\"transpose a column matrix to a row matrix / tensor (dependant on\n",
    "        input to this class on training)\"\"\"\n",
    "        assert len(mat.shape) == 2, 'needs a matrix not a tensor'\n",
    "        newshape = self._metavalues['image_shape']\n",
    "        return mat.T.reshape(mat.shape[1], *newshape)\n",
    "    \n",
    "    def __str__(self):\n",
    "        D = self._metavalues\n",
    "        return f\"NMF({D['components']})\"\n",
    "\n",
    "\n",
    "\n",
    "def l2_norm(arr):\n",
    "    return np.linalg.norm(arr)\n",
    "\n",
    "\n",
    "\n",
    "## was in models.py\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "class TrivialModel:\n",
    "\n",
    "    def __init__(self, rv=0):\n",
    "        self.rv = rv\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        return [self.rv]*len(x)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'TrivialModel({self.rv})'\n",
    "\n",
    "\n",
    "class GNB(TrivialModel):\n",
    "\n",
    "    def __init__(self, sigma_adjust=1e-4):\n",
    "        self.sigma_adjust = sigma_adjust\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.levels = sorted(list(set(y)))\n",
    "        self.means = []\n",
    "        self.sds = []\n",
    "        self.pclasses = []\n",
    "        for i, level in enumerate(sorted(self.levels)):\n",
    "            xcat = x[y==level]\n",
    "            self.means.append(np.mean(xcat, axis=0))\n",
    "            self.sds.append(np.std(xcat, axis=0))\n",
    "            self.pclasses.append(len(xcat) / len(x))\n",
    "        self.means = np.array(self.means)\n",
    "        self.sds = np.array(self.sds)\n",
    "        self.pclasses = np.array(self.pclasses)\n",
    "\n",
    "    def norm_cdf(self, x, mu, sigma):\n",
    "        \"\"\"norm_cdf.\n",
    "\n",
    "        :param x: ndarray (n by k) data\n",
    "        :param mu: ndarray (k by c) means of each feature for each class\n",
    "        :param sigma: ndarray (k by c) sd of each feature for each class\n",
    "        :return: ndarray (n by k by c) probability contribution of each feature being that value.\n",
    "        \"\"\"\n",
    "\n",
    "        n, k = x.shape\n",
    "        c = mu.shape[1]\n",
    "\n",
    "        # sanity checks\n",
    "        assert mu.shape[0] == k\n",
    "        assert mu.shape == sigma.shape\n",
    "\n",
    "        xt = np.transpose(np.tile(x, (c,1,1)), (1, 2, 0))\n",
    "        mut = np.tile(mu, (n, 1, 1))\n",
    "        sigmat = np.tile(sigma, (n, 1, 1)) + self.sigma_adjust\n",
    "\n",
    "        # the above are now in the same shapes, meaning that *, +, -, /, **\n",
    "        # all operate element-wise in a predictable way\n",
    "        #>>> assert xt.shape == (n, k, c), f'{xt.shape} =/= {(n, k, c)}'\n",
    "        #>>> assert mut.shape == (n, k, c), f'{mut.shape} =/= {(n, k, c)}'\n",
    "        #>>> assert sigmat.shape == (n, k, c), f'{sigmat.shape} =/= {(n, k, c)}'\n",
    "\n",
    "        inexp = -(xt - mut)**2 / (2 * sigmat ** 2)\n",
    "        num = np.exp(inexp)\n",
    "        den = np.sqrt(2 * np.pi * sigmat**2)\n",
    "\n",
    "        rv =  num / den\n",
    "        # sigmat can sometimes be 0 with homogenous features (features that\n",
    "        # contribute nothing) This means that p(ci | xi) = 0 as xi xc for all ci.\n",
    "        return rv\n",
    "\n",
    "    def predict(self, x):\n",
    "        # tile and make of the form (n x k x c)\n",
    "        ind_probs = self.norm_cdf(x, self.means.T, self.sds.T)\n",
    "        conditionalprobs = np.sum(np.log(ind_probs), axis=1)\n",
    "\n",
    "        # pclasses has shape (c,), so it is repeated for each row in conditionalprobs * pclasses\n",
    "\n",
    "        return np.argmax(conditionalprobs + np.log(self.pclasses), axis=1)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'GNB()'\n",
    "\n",
    "\n",
    "def cosine_distance(X, Y):\n",
    "    norms = np.linalg.norm(X, axis=1)[:, None] * np.linalg.norm(Y, axis=1)[None, :]\n",
    "    \n",
    "    dot = np.zeros((X.shape[0], Y.shape[0]))\n",
    "    for i, x in enumerate(X):\n",
    "        for j, y in enumerate(Y):\n",
    "            dot[i, j] = x @ y\n",
    "    return 1 - dot / norms\n",
    "\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, k=3, distancefunction='euclidian', weigh_voting=True):\n",
    "        self.k = k\n",
    "        self.distfn = {\n",
    "                'euclidian': lambda x, y: np.linalg.norm(x[:, None, :] - y[None, :, :], axis=2),\n",
    "                'manhatten': lambda x, y:np.sum(np.abs(x[:, None, :] - y[None, :, :]), axis=2),\n",
    "                'cosine': cosine_distance,\n",
    "        }[distancefunction]\n",
    "        self._name = f'{type(self).__name__}(k={k}, distancefunction={distancefunction})'\n",
    "        self.weigh_voting = weigh_voting\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, x, batchsize=10, k=None, dists=None, return_dists=False):\n",
    "        if dists is None:\n",
    "            all_dists = []\n",
    "            for batch in breakup(x, batchsize):\n",
    "\n",
    "                all_dists.append(self.distfn(batch, self.x))\n",
    "            all_dists = np.concatenate(all_dists)\n",
    "        else:\n",
    "            all_dists = dists\n",
    "\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "\n",
    "        import datetime\n",
    "        idxs = np.argpartition(all_dists, k)[:,:k]  # k smallest distance indexes\n",
    "        idxs_2 = np.argsort(all_dists, axis=1)[:,:k]\n",
    "        if not self.weigh_voting:\n",
    "            raise NotImplementedError()\n",
    "            # TODO this method does not work - I must fix it\n",
    "            res = np_mode(self.y[idxs], axis=(1,))\n",
    "        else:\n",
    "            all_dists[:, idxs]  # get k closest.\n",
    "            # distances at those indexes\n",
    "            distance_to = np.array([a[i] for a, i in zip(all_dists, idxs_2)])\n",
    "            # true values at those indexes\n",
    "            guesses = np.array([self.y[i] for i in idxs_2])\n",
    "            # possible outcomes (may be a subset of range(10))\n",
    "            possible = np.array(list(set(guesses.flatten())))\n",
    "            # hardware; dimensions : represent:\n",
    "            # 0 : data that is being predicted\n",
    "            # 1 : possible labels the data could take\n",
    "            # 2 : data that has known labels\n",
    "            # actual values are True if that is the label for that known example\n",
    "            mask = possible[None, :, None] == guesses[:, None, :]\n",
    "            # mask the distances (at correct indecies) with the mask, calculating contributions\n",
    "            # then sum them\n",
    "            distcats = np.sum(mask * 1/distance_to[:, None, :], axis=2)\n",
    "            # calculate the indexes\n",
    "            predidx = distcats.argmax(axis=1)\n",
    "            # collect the indexes\n",
    "            res = possible[predidx]\n",
    "\n",
    "        if return_dists:\n",
    "            return all_dists, res\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._name\n",
    "\n",
    "    \n",
    "def np_mode(a, axis=None):\n",
    "    \"\"\"np_mode - numpy.mode implementation (it is not in base numpy).\n",
    "\n",
    "    Works in the same way as np.mean, but returns the mode instead.\n",
    "\n",
    "    :param ndarray: array to perform mode on\n",
    "    :param axis: axis (or axes) over which to perform the mode.\n",
    "    :return: ndarray of modes\n",
    "\n",
    "    if a.shape == (w, x, y, z) and axis == (1, 2), then the returned value\n",
    "    will have the shape (w, z).\n",
    "    \"\"\"\n",
    "    if axis is not None:\n",
    "        try:\n",
    "            #axis = tuple(set(range(len(a.shape))) - set(axis))\n",
    "            axis = tuple(axis)\n",
    "        except TypeError:\n",
    "            return np_mode(a, axis=(axis,))\n",
    "    options = np.array([np.sum(a == i, axis=axis)\n",
    "        for i in range(np.max((a)))\n",
    "        ])\n",
    "    return np.argmax(options, axis=0)\n",
    "\n",
    "\n",
    "def breakup(itr, batchsize):\n",
    "    \"\"\"break up an iterable into more managable batches\n",
    "\n",
    "    :param itr: iterable to break up\n",
    "    :param batchsize: size of batch to break up into\n",
    "\n",
    "    This will generate batches (of length batchsize) of iterable.\n",
    "    \"\"\"\n",
    "    for batch in count():\n",
    "        batch *= batchsize\n",
    "        n = batch + batchsize\n",
    "        if n < len(itr):\n",
    "            yield itr[batch:n]\n",
    "        else:\n",
    "            break\n",
    "    yield itr[batch:]\n",
    "\n",
    "\n",
    "### was in model_tools.py\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import h5py\n",
    "import operator as op\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def __init__(self, transformations):\n",
    "        self._transformations = transformations\n",
    "\n",
    "    def fit(self, X, Y, verbose=False):\n",
    "        \"\"\" fit transformations to data and train the model with the output \"\"\"\n",
    "        for name, model in self._transformations[:-1]:\n",
    "            if verbose:\n",
    "                print(f'fitting transformation {name} (={model})')\n",
    "            model.fit(X)\n",
    "            X = model.transform(X)\n",
    "        if verbose:\n",
    "            print(f'fitting estimator {name} (={model})')\n",
    "\n",
    "        self._transformations[-1][1].fit(X, Y)\n",
    "\n",
    "    def run_transform(self, data, verbose=False):\n",
    "        for name, model in self._transformations[:-1]:\n",
    "            if verbose:\n",
    "                print(f'transforming with {name} (={model})')\n",
    "            data = model.transform(data)\n",
    "        return data\n",
    "\n",
    "    def predict(self, X, verbose=False, **pred_kwargs):\n",
    "        return self._transformations[-1][1].predict(\n",
    "                self.run_transform(X, verbose=verbose), **pred_kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Pipeline({\", \".join(map(op.itemgetter(0), self._transformations))})'\n",
    "\n",
    "\n",
    "class CrossValidateClassification:\n",
    "    \"\"\"CrossValidateClassification.\n",
    "\n",
    "    run cross-validation on a dataset with multiple models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, n: int = 10, verbose: bool = False):\n",
    "        \"\"\"__init__.\n",
    "\n",
    "        if len(data) > len(labels) data is cut short to only include the first len(labels) examples.\n",
    "\n",
    "        :param data: input data (all data) MxN\n",
    "        :type data: np.ndarray\n",
    "        :param labels: input labels (all labels) N\n",
    "        :type labels: np.ndarray\n",
    "        :param n:\n",
    "        :type n: int\n",
    "        :return: [(true labels, predicted labels), ...]\n",
    "        \"\"\"\n",
    "\n",
    "        idxs = list(range(len(labels)))\n",
    "        random.shuffle(idxs)\n",
    "        size = len(idxs)//n + 1\n",
    "        validations = [set(idxs[i:i+size]) for i in range(0, len(idxs), size)]\n",
    "        idxs = set(idxs)\n",
    "        self.validation_groups = tuple(np.array(list(x)) for x in validations)\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.verbose = bool(verbose)\n",
    "\n",
    "    def run_validation(self, model: object) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"run_validation.\n",
    "\n",
    "        :param model_class: called to create the model. Must define fit and predict methods.\n",
    "        :type model_class: object\n",
    "        :param args: arguments passed to model_class on instanciation.\n",
    "        :param kwargs: keyword arguments passed to model_class on instanciation.\n",
    "        :return: list of tuples of predicted observations and true observations\n",
    "        :rtype: List[Tuple[np.ndarray, np.ndarray]]\n",
    "        \"\"\"\n",
    "        res_true = []\n",
    "        res_pred = []\n",
    "        idxs = set(list(range(len(self.labels))))\n",
    "        for i, idx_test in enumerate(self.validation_groups):\n",
    "            if self.verbose:\n",
    "                print(f'running fold #{i+1}     ')\n",
    "\n",
    "            idx_train    = np.array(list(idxs - set(idx_test)))\n",
    "\n",
    "            train_data   = self.data[idx_train]\n",
    "            train_labels = self.labels[idx_train]\n",
    "\n",
    "            test_data    = self.data[idx_test]\n",
    "            test_labels  = self.labels[idx_test]\n",
    "\n",
    "            model.fit(train_data, train_labels)\n",
    "            pred = model.predict(test_data)\n",
    "            res_true.append(test_labels)\n",
    "            res_pred.append(pred)\n",
    "        return res_true, res_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def metrics(true, predicted=None, names=('accuracy',)):\n",
    "        \"\"\"metrics.\n",
    "\n",
    "        :param true:\n",
    "        :param predicted:\n",
    "        \"\"\"\n",
    "\n",
    "        if predicted is None:  # attempt to unpack\n",
    "            true, predicted = true\n",
    "\n",
    "        cm = confusion_matrix(true, predicted)\n",
    "        acc = np.mean(true == predicted)\n",
    "        res = []\n",
    "        if 'accuracy' in names:\n",
    "            res.append(np.mean(true == predicted))\n",
    "        return acc, cm  # prec, recall\n",
    "\n",
    "    @staticmethod\n",
    "    def aggregate_metrics(true, predicted):\n",
    "        return np.array([CrossValidateClassification.metrics(t, p) for t, p in zip(true, predicted)])\n",
    "\n",
    "    def _random_idxs(self, n):\n",
    "        idxs = list(range(len(self.labels)))\n",
    "        random.shuffle(idxs)\n",
    "        return idxs[:n]\n",
    "\n",
    "    def random_sample(self, n):\n",
    "        idxs = self._random_idxs(n)\n",
    "        return self.data[idxs], self.labels[idxs]\n",
    "\n",
    "\n",
    "def confusion_matrix(true, pred):\n",
    "    rv = np.zeros([len(true)]*2)\n",
    "    for t, p in zip(true, pred):\n",
    "        rv[t, p] += 1\n",
    "    return rv\n",
    "\n",
    "\n",
    "def plot_confusion(confusion_matrix, title=None, labels=None, cmap='YlGnBu'):\n",
    "    ax = sns.heatmap(confusion_matrix, linewidth=0.2, annot=True, cmap=cmap, square=True)\n",
    "    if labels is not None:\n",
    "        ax.set_xticklabels(labels, rotation=90)\n",
    "        ax.set_yticklabels(labels, rotation=0)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "        \n",
    "class ModelRunner:\n",
    "\n",
    "    def __init__(self, *models):\n",
    "        self.models = models\n",
    "\n",
    "    def load_data(self, return_all=False):\n",
    "        \"\"\" loads the datasets provided for this assignment \"\"\"\n",
    "        if hasattr(self, 'xtr'):\n",
    "            return  # has already been run\n",
    "\n",
    "        with h5py.File('./Input/train/images_training.h5','r') as H:\n",
    "            self.xtr = np.copy(H['datatrain'])\n",
    "\n",
    "        with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
    "            self.ytr = np.copy(H['labeltrain'])\n",
    "\n",
    "        with h5py.File('./Input/test/labels_testing_2000.h5', 'r') as H:\n",
    "            self.yte = np.copy(H['labeltest'])\n",
    "\n",
    "        with h5py.File('./Input/test/images_testing.h5', 'r') as H:\n",
    "            if return_all:\n",
    "                self.xte = np.copy(H['datatest'])\n",
    "            else:\n",
    "                self.xte = np.copy(H['datatest'])[:len(self.yte)]\n",
    "\n",
    "    def run_cv(self, folds=10, verbose=False):\n",
    "        \"\"\" runs n-fold cross validation on the model \"\"\"\n",
    "        self.load_data()\n",
    "        validator = CrossValidateClassification(self.xtr, self.ytr, n=folds, verbose=verbose)\n",
    "        results = {}\n",
    "        for model in self.models:\n",
    "            if verbose:\n",
    "                print(f'running {model}')\n",
    "            true, pred = validator.run_validation(model)\n",
    "            results[str(model)] = CrossValidateClassification.aggregate_metrics(true, pred)\n",
    "        return results\n",
    "\n",
    "    def run(self, n=None, verbose=False):\n",
    "        \"\"\"run all models and evaluate performance\n",
    "\n",
    "        :param n: subset of test samples to evaluate model on.\n",
    "        :param verbose: if true, print progress.\n",
    "        \"\"\"\n",
    "        self.load_data()\n",
    "        results = {}\n",
    "        try:\n",
    "            if n is None:\n",
    "                n = len(self.xte)\n",
    "        except ValueError:\n",
    "            raise ValueError('could not interperate input')\n",
    "        for model in self.models:\n",
    "            if verbose:\n",
    "                print(f'running {model}')\n",
    "            try:\n",
    "                model.fit(self.xtr, self.ytr)\n",
    "            except Exception:\n",
    "                tb = traceback.format_exc()\n",
    "                warnings.warn(f'{tb}\\nmodel {model} exited unexpectedly.'\n",
    "                        '\\nskipping...')\n",
    "                continue  # skip this model\n",
    "            results[str(model)] = CrossValidateClassification.metrics(\n",
    "                    self.yte[:n], model.predict(self.xte[:n])\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f'{model} got {results[str(model)]}')\n",
    "        return results\n",
    "\n",
    "\n",
    "def confusion_matrix(true, pred, labels=None):\n",
    "    if labels is None:\n",
    "        labels = list(set(true))\n",
    "    rv = np.zeros([len(labels)]*2)\n",
    "    for t, p in zip(true, pred):\n",
    "        rv[labels.index(p), labels.index(t)] += 1\n",
    "    return rv\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(mat, labels):\n",
    "    '''\n",
    "    :param mat: confusion matrix\n",
    "    :param labels: ordered labels to show on graph\n",
    "    '''\n",
    "    plt.imshow(mat)\n",
    "    plt.xticks(range(len(labels)), labels, rotation='vertical')\n",
    "    plt.yticks([-0.5] + list(range(len(labels))) + [len(labels) - .5],\n",
    "               [''] + list(labels) + [''],\n",
    "               rotation='horizontal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_predict = Pipeline([\n",
    "    ('PCA(64, False)', PCA(64, normalize=False)),\n",
    "    ('KNN(6, Manhatten)', KNN(6, 'manhatten', weigh_voting=True))\n",
    "])\n",
    "# using the whole dataset to train\n",
    "mr = ModelRunner()\n",
    "mr.load_data(True)\n",
    "# concatenate \n",
    "xtr, ytr, xte, yte, xpred = mr.xtr, mr.ytr, mr.xte[:len(mr.yte)], mr.yte, mr.xte[len(mr.yte):]\n",
    "best_model_predict.fit(np.concatenate((xtr, xte)), np.concatenate((ytr, yte)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = Pipeline([\n",
    "    ('PCA(64, False)', PCA(64, normalize=False)),\n",
    "    ('KNN(6, Manhatten)', KNN(6, 'manhatten', weigh_voting=True))\n",
    "])\n",
    "bm.fit(xtr, ytr)\n",
    "preds = bm.predict(xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f'Confusion matrix of best selected model\\nAccuracy={np.mean(preds == yte)}\\n{bm}')\n",
    "plot_confusion_matrix(confusion_matrix(yte, preds), labels=['T-shirt/top','Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model_predict.predict(xpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to h5 file\n",
    "file = h5py.File('./Output/predicted_labels.h5', 'w')\n",
    "file.create_dataset('predictions', data=predictions)\n",
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
